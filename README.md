# UPB RAG Career Exploration Assistant

A Retrieval-Augmented Generation (RAG) system to help prospective students explore UPB's engineering programs conversationally in Spanish. The system uses manually curated markdown documents and provides multi-strategy retrieval for accurate, context-aware responses.

## Features

- Conversational AI with GPT-4o-mini for natural interactions
- Multi-strategy retrieval: BM25, Vector Similarity, MMR, and Hybrid RRF
- Header-based chunking with semantic structure preservation
- Rich metadata extraction from YAML frontmatter
- Conversation memory for multi-turn dialogues
- Source citations for transparency
- Spanish language optimized
- 17 curated documents covering 12 engineering programs plus metadata catalog
- 368 optimized chunks with hierarchical metadata for efficient retrieval

## Tech Stack

| Component | Technology |
|-----------|-----------|
| **LLM** | Azure ChatOpenAI / OpenAI GPT-4o-mini |
| **Embeddings** | AzureOpenAIEmbeddings (text-embedding-3-small) |
| **Vector Store** | FAISS (CPU version) |
| **Framework** | LangChain 1.0.2 |
| **Retrieval** | BM25 (rank-bm25) + MMR + RRF Ensemble |
| **UI** | Gradio *(planned)* |
| **Deployment** | Hugging Face Spaces *(planned)* |
| **Package Manager** | UV |

## Project Structure

```
.
â”œâ”€â”€ data/                      # Curated markdown content
â”‚   â”œâ”€â”€ about_upb.md          # University information
â”‚   â”œâ”€â”€ contact/              # Contact information
â”‚   â”œâ”€â”€ engineerings/         # Engineering program details (12 programs)
â”‚   â”œâ”€â”€ enroll/               # Enrollment information
â”‚   â”œâ”€â”€ metadata/             # Program metadata catalog
â”‚   â”‚   â””â”€â”€ metadata.json    # Structured program information
â”‚   â””â”€â”€ scholarships/         # Financial aid & scholarships
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â””â”€â”€ embeddings.py    # Azure/OpenAI embeddings initialization
â”‚   â”œâ”€â”€ loader/
â”‚   â”‚   â””â”€â”€ ingest.py        # Document loader with metadata.json support
â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â””â”€â”€ chunking.py      # Header-based chunking with YAML frontmatter
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â””â”€â”€ chain.py         # RAG chain with conversation memory
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â””â”€â”€ retriever.py     # Multi-strategy retriever (BM25/MMR/Hybrid)
â”‚   â”œâ”€â”€ vectorstore/
â”‚   â”‚   â””â”€â”€ store.py         # FAISS vector store manager
â”‚   â”œâ”€â”€ pipeline.py          # Document preparation pipeline
â”‚   â””â”€â”€ setup_retrieval.py   # Complete retrieval system setup
â”‚
â”œâ”€â”€ vectorstore/             # FAISS index files (gitignored)
â””â”€â”€ pyproject.toml           # Dependencies (UV)
```

## ðŸš€ Getting Started

### Prerequisites

- Python 3.12
- [UV](https://docs.astral.sh/uv/) package manager


### Installation

```bash
# Clone the repository
git clone https://github.com/Rosvend/UPBot.git
cd UPBot

# Install dependencies with UV
uv sync
```

### Configuration

Create a `.env` file with your Azure OpenAI credentials (for embeddings & LLM):

```env
AZURE_OPENAI_API_KEY=your_key_here
AZURE_OPENAI_ENDPOINT=https://your-endpoint.openai.azure.com/
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-small
AZURE_OPENAI_LLM_DEPLOYMENT=gpt-4o-mini
```

### Running the Pipeline

#### 1. **RAG Chain with Conversation** (Full System)
```bash
# Test complete RAG chain with GPT-4o-mini, memory, and source citations
uv run python src/rag/chain.py
```
**What it does**:
- Sets up complete retrieval system
- Initializes GPT-4o-mini LLM
- Tests multi-turn conversation
- Shows source citations
- Demonstrates conversation memory

**Output**: Complete conversational RAG system test

#### 2. **Complete Retrieval Setup**
```bash
# Set up embeddings, vector store, and all retrieval methods
uv run python src/setup_retrieval.py
```
**What it does**:
- Loads 16 markdown documents
- Creates ~217 optimized chunks
- Initializes Azure OpenAI embeddings
- Creates/loads FAISS vector store
- Tests all retrieval methods (BM25, Similarity, MMR, Hybrid)

**Output**: Fully initialized retrieval system ready for RAG

#### 3. **Individual Modules**

**Load Documents**
```bash
uv run python src/loader/ingest.py
```
**Output**: 16 documents with category metadata

**Chunk Documents**
```bash
uv run python src/processing/chunking.py
```
**Output**: ~217 chunks (avg 792 chars)

**Test Embeddings**
```bash
uv run python src/embeddings/embeddings.py
```
**Output**: Embedding model initialization test

**Test Vector Store**
```bash
uv run python src/vectorstore/store.py
```
**Output**: FAISS index creation, save, and load test

**Test Retrieval**
```bash
uv run python src/retrieval/retriever.py
```
**Output**: BM25 retrieval test (no embeddings needed)


## Quick Start Examples

### Interactive Chat
```bash
# Run interactive chat interface
uv run python src/example_usage.py
```

Type your questions in Spanish and the assistant will respond using the RAG system. Commands:
- `salir` - Exit the chat
- `limpiar` - Clear conversation history

### Programmatic Usage

**Basic RAG Query**:
```python
from setup_retrieval import setup_retrieval_system
from rag.chain import UPBRAGChain

# Initialize
retriever, _, _ = setup_retrieval_system()
rag_chain = UPBRAGChain(retriever, retrieval_method="hybrid")

# Ask question
response = rag_chain.invoke("Â¿QuÃ© es la ingenierÃ­a de sistemas?")
print(response['answer'])
```

**With Source Citations**:
```python
response = rag_chain.invoke(
    "Â¿QuÃ© becas estÃ¡n disponibles?",
    include_sources=True
)

print(response['answer'])
print("\nFuentes:")
for source in response['sources']:
    print(f"- {source['category']}: {source['source']}")
```

**Multi-turn Conversation**:
```python
# First question
r1 = rag_chain.invoke("Â¿QuÃ© ingenierÃ­as tienen?")

# Follow-up (uses conversation memory)
r2 = rag_chain.invoke("Â¿CuÃ¡l me recomiendas si me gusta programar?")

# Another follow-up
r3 = rag_chain.invoke("Â¿CuÃ¡nto dura ese programa?")

# Clear history when done
rag_chain.clear_history()
```

